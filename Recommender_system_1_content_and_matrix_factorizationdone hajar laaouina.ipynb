{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6975cef0",
      "metadata": {
        "id": "6975cef0"
      },
      "outputs": [],
      "source": [
        "# we set a random seed here to make the results in this notebook reproducible\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "my_seed = 0\n",
        "random.seed(my_seed)\n",
        "np.random.seed(my_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126d08e7",
      "metadata": {
        "id": "126d08e7"
      },
      "source": [
        "# Evaluating Recommender Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "405decfe",
      "metadata": {
        "id": "405decfe"
      },
      "source": [
        "The evaluation of recommender systems follows the same paradigm found in general machine learning. We need a development set that we use for training algorithms and evaluating changes in hyperparameter settings, either via a holdout split or cross-validation. Finally, we need a test set that was neither part of training nor parameter optimization for an unbiased evaluation of our recommender system.\n",
        "\n",
        "By default the Surprise library supports RMSE(Root Mean Squared Error), MSE(Mean Squared Error) and MAE(Mean Absolute Error) as known from the lecture. The following cell demonstrates how we can create a train/test split for the MovieLens dataset, train a KNNBasic user-based recommender and subsequently evaluate its performance on the test set.\n",
        "\n",
        "We will once gain use the [**MovieLens-100k**](https://grouplens.org/datasets/movielens/) dataset we already saw in the last exercise. This time we will load the dataset from local files as we will later want to make use of content features of the movies which are not available when using the dataset auto loading function in surprise. In the project you can use the same loading function for your own datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "490311c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "490311c1",
        "outputId": "ee86d899-2f3f-4a35-895c-c780a4ddf686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: surprise in /usr/local/lib/python3.10/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.10/dist-packages (from surprise) (1.1.3)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.10.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  userId movieId  rating  timestamp\n",
              "0      1       1     4.0  964982703\n",
              "1      1       3     4.0  964981247\n",
              "2      1       6     4.0  964982224\n",
              "3      1      47     5.0  964983815\n",
              "4      1      50     5.0  964982931"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9a3748b-cced-45e1-aff0-462ec8d451f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964981247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964983815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964982931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9a3748b-cced-45e1-aff0-462ec8d451f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d9a3748b-cced-45e1-aff0-462ec8d451f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d9a3748b-cced-45e1-aff0-462ec8d451f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install surprise\n",
        "import surprise\n",
        "from surprise import Dataset, Reader, KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "## Load movies data\n",
        "movies = pd.read_csv('movies.csv')\n",
        "#tags = pd.read_csv('ml-latest-small/tags.csv') # Optionally add tags\n",
        "ratings = pd.read_csv('ratings.csv', dtype= {'userId': str, 'movieId': str})\n",
        "links = pd.read_csv('links.csv')\n",
        "display(ratings.head())\n",
        "\n",
        "# Load data into dataset\n",
        "reader = Reader(rating_scale=(0, 5))\n",
        "data = Dataset.load_from_df(ratings[['userId','movieId','rating']], reader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79131633",
      "metadata": {
        "id": "79131633"
      },
      "source": [
        "### Holdout Splits for Evaluation\n",
        "\n",
        "Similar to the train_test split function from scikit-learn, the surprise package offers a [train_test_split method](https://surprise.readthedocs.io/en/stable/model_selection.html#surprise.model_selection.split.train_test_split). The splitting is done randomly. The following example splits 25% of ratings into the test fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "acb74dfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acb74dfb",
        "outputId": "1c9dbb83-944e-465c-a30e-c7a711a61162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        }
      ],
      "source": [
        "from pdb import set_trace\n",
        "\n",
        "# sample random trainset and testset\n",
        "# test set is made of 25% of the ratings.\n",
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=my_seed)\n",
        "\n",
        "# Define a custom similarity function for the KNNBasic algorithm\n",
        "sim_options = {'name': 'pearson', 'user_based': True, 'min_support': 1}\n",
        "\n",
        "algo = KNNBasic(sim_options=sim_options)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336602f9",
      "metadata": {
        "id": "336602f9"
      },
      "source": [
        "### Numerical Evaluation\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "The numerical ratings (Root) Mean Squared Error (MSE, RMSE) and Mean Absolute Error (MAE) are implemented in the [accuracy package](https://surprise.readthedocs.io/en/stable/accuracy.html) of the surprise library and can directly be calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "52476624",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52476624",
        "outputId": "80686267-0d16-404c-9aa1-5972e2f71bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.9390\n",
            "RMSE: 0.9690\n",
            "MAE:  0.7481\n"
          ]
        }
      ],
      "source": [
        "from surprise.accuracy import mse, rmse, mae\n",
        "# Compute MSE, RMSE and MAE on the test set predictions\n",
        "mse(predictions)\n",
        "rmse(predictions)\n",
        "mae(predictions);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b04181",
      "metadata": {
        "id": "a0b04181"
      },
      "source": [
        "### Categorical Evaluation\n",
        "\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "Categorical Ratings, well known from Data Mining, like **Precision**, **Recall** and **F1** are not directly implemented in the Surprise library. But we can gather the relvant information ourselves and calculate the measures. It is important to note that we have to set a manual rating threshold to indicate when a movie is actually relevant for a user. Setting a global rating as we do in the following example is a very simple approach which does not take different rating-behaviour of users into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9fb181d8",
      "metadata": {
        "id": "9fb181d8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "def precision_recall_f1(predictions, threshold=3.5):\n",
        "    \"\"\"Return precision, recall and f1 metrics averaged across all users\"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    # collect metrics per user\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    f1s = dict()\n",
        "    accuracies = dict()\n",
        "\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # get relevance labels for average precision calculation\n",
        "        y_true = [1 if (true_r >= threshold) else 0 for (_, true_r) in user_ratings]\n",
        "        y_pred = [1 if (est_r >= threshold) else 0 for (est_r, _) in user_ratings]\n",
        "\n",
        "        precisions[uid] = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recalls[uid] = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1s[uid] = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        accuracies[uid] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # average scores over all users\n",
        "    avg_precision = sum(list(precisions.values())) / len(list(precisions.values()))\n",
        "    avg_recall = sum(list(recalls.values())) / len(list(recalls.values()))\n",
        "    avg_f1 = sum(list(f1s.values())) / len(list(f1s.values()))\n",
        "\n",
        "    avg_accuracy = sum(list(accuracies.values())) / len(list(accuracies.values()))\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "888bf22a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "888bf22a",
        "outputId": "f4d18f0e-e3ea-4ff0-9c4d-056fc8410200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. Precision: 0.7340483684425808\n",
            "Avg. Recall: 0.7227663637153214\n",
            "Avg. F1: 0.7036871120873112\n",
            "Avg. Accuracy: 0.6701322626336957\n"
          ]
        }
      ],
      "source": [
        "# get average evaluation measures and print them\n",
        "avg_precision, avg_recall, avg_f1, avg_accuracy = precision_recall_f1(predictions, threshold=3.5)\n",
        "print(f'Avg. Precision: {avg_precision}')\n",
        "print(f'Avg. Recall: {avg_recall}')\n",
        "print(f'Avg. F1: {avg_f1}')\n",
        "print(f'Avg. Accuracy: {avg_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd398d35",
      "metadata": {
        "id": "bd398d35"
      },
      "source": [
        "### Rank-based Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "030164f6",
      "metadata": {
        "id": "030164f6"
      },
      "source": [
        "In real-world use-cases just evaluating on all recommended items may severly overestimate the performance of a recommender system as the user is usually presented with a ranked list of recommendations. If the relevant items are recommended but appear at the bottom of the list, this can be misleading as people do not usually scroll through all the recommendations.\n",
        "\n",
        "To incorporate the rank of an item we can make use of rank based evaluation metrics like Average Precision (AP), Precision@k, R-Precision and nDCG\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "36bf13d4",
      "metadata": {
        "id": "36bf13d4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import average_precision_score, ndcg_score\n",
        "\n",
        "def measures_at_k(predictions, k=10, threshold=3.5):\n",
        "    \"\"\"Return precision and nDCG at k metrics averaged across all users\"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    average_precisions = dict()\n",
        "    precisions_at_k = dict()\n",
        "    ndcgs_at_k = dict()\n",
        "\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # get relevance labels for average precision calculation\n",
        "        y_true = [1 if (true_r >= threshold) else 0 for (_, true_r) in user_ratings]\n",
        "        y_pred = [1 if (est_r >= threshold) else 0 for (est_r, _) in user_ratings]\n",
        "\n",
        "        y_true_at_k = y_true[:k]\n",
        "        y_pred_at_k = y_pred[:k]\n",
        "\n",
        "        if sum(y_true) > 0:\n",
        "            average_precisions[uid] = average_precision_score(y_true, y_pred)\n",
        "        else:\n",
        "            average_precisions[uid] = 0\n",
        "\n",
        "        precisions_at_k[uid] = precision_score(y_true_at_k, y_pred_at_k, zero_division=0)\n",
        "\n",
        "        if not len(user_ratings) == 1:\n",
        "            ndcgs_at_k[uid] = ndcg_score(np.asarray([[x for (_, x) in user_ratings]]), np.asarray([[x for (x, _) in user_ratings]]), k=k)\n",
        "        elif len(user_ratings) == 1:\n",
        "            ndcgs_at_k[uid] = sum(\n",
        "            ((true_r >= threshold) and (est >= threshold))\n",
        "            for (est, true_r) in user_ratings\n",
        "        )\n",
        "\n",
        "        # compute simple averages over all users for each score:\n",
        "        avg_average_precisions = sum(val for val in average_precisions.values()) / len(average_precisions)\n",
        "        avg_precisions_at_k = sum(val for val in precisions_at_k.values()) / len(precisions_at_k)\n",
        "        avg_ndcgs_at_k = sum(val for val in ndcgs_at_k.values()) / len(ndcgs_at_k)\n",
        "\n",
        "    return avg_average_precisions, avg_precisions_at_k, avg_ndcgs_at_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "199b2d9c",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "199b2d9c",
        "outputId": "39d9c281-63e9-40fb-f118-960383856b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. AveragePrecision: 0.7174516127478939\n",
            "Avg. Precision@5: 0.7884699453551931\n",
            "Avg. nDCG@5: 0.8744889687841513\n"
          ]
        }
      ],
      "source": [
        "# get average evaluation measures and print them\n",
        "avg_average_precisions, avg_precisions, avg_ndcgs = measures_at_k(predictions, k=5, threshold=3.5)\n",
        "print(f'Avg. AveragePrecision: {avg_average_precisions}')\n",
        "print(f'Avg. Precision@5: {avg_precisions}')\n",
        "print(f'Avg. nDCG@5: {avg_ndcgs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719cfd13",
      "metadata": {
        "id": "719cfd13"
      },
      "source": [
        "# Model-based Recommender Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d21cb0",
      "metadata": {
        "id": "51d21cb0"
      },
      "source": [
        "The [SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD) algorithm was popularized during the Netflix competition and is based on matrix factorization. The predicition is set as:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The method applies stochastic gradient descent to minimize the regularized squared error:\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "Due to stepwise gradient-descent based optimization the method offers a lot of hyperparameters like number of epochs, learning rate, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a5677e5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5677e5d",
        "outputId": "1454c3be-70dc-4b8c-bf29-86e489eacbc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.7859\n",
            "RMSE: 0.8865\n",
            "MAE:  0.6866\n"
          ]
        }
      ],
      "source": [
        "from surprise import SVD\n",
        "\n",
        "algo = SVD(n_epochs=10, lr_all=0.005, reg_all=0.4)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Then compute MSE, RMSE and MAE\n",
        "mse(predictions)\n",
        "rmse(predictions)\n",
        "mae(predictions);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa11a6e",
      "metadata": {
        "id": "0fa11a6e"
      },
      "source": [
        "# Content-based Recommender Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929b3bce",
      "metadata": {
        "id": "929b3bce"
      },
      "source": [
        "All previously applied methods are collaborative methods and do not take the actual content of the items into account. Usually collaborative recommenders work better when a lot of ratings are available but consider the case where a new movie is introduced to the selection of a streaming company for which no ratings are available yet (cold-start problem). In such cases it can be useful to base recommendations for this movie on content features, e.g. recommend a new Star Wars movie to people that have watched previous installments or are generally interested in the genre.\n",
        "\n",
        "Surprise does not support content-based recommendation by default, but we can extend it and implement our own algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a979dbbd",
      "metadata": {
        "id": "a979dbbd"
      },
      "outputs": [],
      "source": [
        "from gensim.utils import tokenize\n",
        "# Prepare Content\n",
        "movies['content'] = movies['title'] + ' ' + movies['genres'].str.replace('|', ' ', regex=False)\n",
        "movie_content_dict = dict(zip(movies['movieId'], movies['content']))\n",
        "\n",
        "# Add content to trainset to parse the content of the movies to the trainset\n",
        "#Convert content to list - Make sure that starting from 0 to the highest movieId a content entry exists in the list\n",
        "trainset.content = [movie_content_dict[key] if key in movie_content_dict else '' for key in range(0, max(movie_content_dict.keys()))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "193978eb",
      "metadata": {
        "id": "193978eb"
      },
      "outputs": [],
      "source": [
        "# Content-based Prediction using the Gensim library\n",
        "from surprise import AlgoBase, Dataset, PredictionImpossible\n",
        "from surprise.model_selection import cross_validate\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.similarities import SparseMatrixSimilarity\n",
        "\n",
        "class ContentKNN(AlgoBase):\n",
        "    def __init__(self,k=5):\n",
        "\n",
        "        # Always call base method before doing anything.\n",
        "        AlgoBase.__init__(self)\n",
        "        self.k=k\n",
        "\n",
        "    def fit(self, trainset):\n",
        "\n",
        "        # Here again: call base method before doing anything.\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        # Tokenize content by whitespace\n",
        "        tokenized_content = [list(tokenize(line)) for line in trainset.content]\n",
        "\n",
        "        # Compute TF-IDF Similarity Matrix\n",
        "        dct = Dictionary(tokenized_content)  # fit dictionary\n",
        "        self.corpus = [dct.doc2bow(line) for line in tokenized_content]\n",
        "        self.model = TfidfModel(self.corpus)\n",
        "\n",
        "        # Prepare Index\n",
        "        self.index = SparseMatrixSimilarity(self.model[self.corpus], num_features=len(dct))\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "\n",
        "        # The user must be \"known\" as we need at least 1 rated item to make a recommendation\n",
        "        if self.trainset.knows_user(u):\n",
        "            # Get user ratings\n",
        "            user_ratings = self.trainset.ur[u]\n",
        "\n",
        "            # if an item is not \"known\" to the training set (cold-start) we still can\n",
        "            # make a prediction with the content-based recommender\n",
        "            if type(i) is str:\n",
        "                i = int(i.replace('UKN__', '')) - 1\n",
        "\n",
        "            # Get similarities\n",
        "            similarities = self.index[self.corpus[i]]\n",
        "\n",
        "            # Filter user similarity scores by number of nearst neighbours and threshold > 0\n",
        "            sim_user_dict = {user_rating[0]: similarities[user_rating[0]] for user_rating in user_ratings}\n",
        "            sorted_filtered_sim = sorted(list(sim_user_dict.values()), reverse=True)[0:self.k]\n",
        "            sim_user_dict = {user: sim for user, sim in sim_user_dict.items() if sim > 0 and sim in sorted_filtered_sim}\n",
        "\n",
        "            #Simple prediction\n",
        "            if len(sim_user_dict) > 0:\n",
        "                rating = sum([sim_user_dict[user_rating[0]]*user_rating[1] for user_rating in user_ratings if user_rating[0] in sim_user_dict])/\\\n",
        "                            sum([sim_user_dict[user_rating[0]] for user_rating in user_ratings if user_rating[0] in sim_user_dict])\n",
        "            else:\n",
        "                raise PredictionImpossible(\"No similar items found.\")\n",
        "        else:\n",
        "            raise PredictionImpossible(\"User is unknown.\")\n",
        "\n",
        "\n",
        "        return rating"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb98ecd4",
      "metadata": {
        "id": "cb98ecd4"
      },
      "source": [
        "Subsequently we can use the Content-based recommender the same way as any other surprise algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9d3d6f7",
      "metadata": {
        "scrolled": true,
        "id": "a9d3d6f7"
      },
      "outputs": [],
      "source": [
        "algo = ContentKNN(k=5)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Then compute MSE, RMSE and MAE\n",
        "mse(predictions)\n",
        "rmse(predictions)\n",
        "mae(predictions);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c473c7",
      "metadata": {
        "id": "22c473c7"
      },
      "source": [
        "## Cold-start problem\n",
        "Content-based recommender have an advantage over collaborative filtering based approaches, because they can predict ratings for items that have not been rated, yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6579cf",
      "metadata": {
        "id": "6f6579cf"
      },
      "outputs": [],
      "source": [
        "# Cold-start problem\n",
        "uid = str(28)\n",
        "iid = str(1572) # This item has not been rated\n",
        "\n",
        "algo = ContentKNN(k=5)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings of a user for an item that has no ratings in the training set\n",
        "algo.fit(trainset)\n",
        "pred = algo.predict(uid, iid, r_ui=2.5, verbose=True)\n",
        "\n",
        "\n",
        "algo = KNNBasic(sim_options=sim_options)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings of a user for an item that has no ratings in the training set\n",
        "algo.fit(trainset)\n",
        "pred = algo.predict(uid, iid, r_ui=2.5, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "913e0774",
      "metadata": {
        "id": "913e0774"
      },
      "source": [
        "# Hybrid Recommender Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26968d3b",
      "metadata": {
        "id": "26968d3b"
      },
      "source": [
        "Content-based recommenders often work better when there are few ratings available for a user or item. On the other hand, collaborative recommenders work really well when there are a lot of similar users or similarly rated items. Since both cases are common in real-world use cases, it makes sense to build a hybrid recommender system that takes both types of recommendations into account.\n",
        "\n",
        "A simple way to combine different types of recommender systems can be achieved by simply averaging the respective recommended ratings of each recommender. The following cell implements such a hybrid using collaborative filtering with KNNBasic and our self-implemented content-based recommender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73a47916",
      "metadata": {
        "id": "73a47916"
      },
      "outputs": [],
      "source": [
        "class ContentKNNBasicHybrid(AlgoBase):\n",
        "    def __init__(self, k=5, name='pearson', user_based=True, min_support=1):\n",
        "\n",
        "        # Always call base method before doing anything.\n",
        "        AlgoBase.__init__(self)\n",
        "        self.k = k\n",
        "        self.name = name\n",
        "        self.user_based = user_based\n",
        "        self.min_support = min_support\n",
        "\n",
        "    def fit(self, trainset):\n",
        "\n",
        "        # Here again: call base method before doing anything.\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        self.content = ContentKNN(k=5)\n",
        "        self.content.fit(trainset)\n",
        "\n",
        "        sim_options = {'name': self.name, 'user_based': self.user_based, 'min_support': self.min_support}\n",
        "        self.knn = KNNBasic(sim_options=sim_options)\n",
        "        self.knn.fit(trainset)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "        # Implement estimation\n",
        "\n",
        "        if self.trainset.knows_user(u):\n",
        "            r_content = self.content.estimate(u, i)\n",
        "            r_knn = self.knn.estimate(u, i)\n",
        "\n",
        "            #Simple prediction using the average of both ratings\n",
        "            rating = (r_content + r_knn[0]) / 2\n",
        "        else:\n",
        "            raise PredictionImpossible(\"User is unknown.\")\n",
        "\n",
        "        return rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "37a7cbed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37a7cbed",
        "outputId": "6fda6191-e534-4ec9-c118-816f8e09458a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "MSE: 0.9080\n",
            "RMSE: 0.9529\n",
            "MAE:  0.7404\n"
          ]
        }
      ],
      "source": [
        "algo = ContentKNNBasicHybrid(k=5, name='pearson', user_based=True, min_support=1)\n",
        "\n",
        "# Train the algorithm on the trainset, and predict ratings for the testset\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# Then compute MSE, RMSE and MAE\n",
        "mse(predictions)\n",
        "rmse(predictions)\n",
        "mae(predictions);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1b8dd6",
      "metadata": {
        "id": "3b1b8dd6"
      },
      "source": [
        "Comparing these results to the previous results of the KNNBasic and Content-based recommenders in isolation, we can clearly see an improvement across all three numerical measures.\n",
        "\n",
        "KNNBasic:\n",
        "MSE: 0.9390\n",
        "RMSE: 0.9690\n",
        "MAE:  0.7481\n",
        "\n",
        "Content:\n",
        "MSE: 1.0619\n",
        "RMSE: 1.0305\n",
        "MAE:  0.8012\n",
        "\n",
        "Hybrid:\n",
        "MSE: 0.9080\n",
        "RMSE: 0.9529\n",
        "MAE:  0.7404"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b12d6d6",
      "metadata": {
        "id": "2b12d6d6"
      },
      "source": [
        "# Generation of example recommendations for a user"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a09a53",
      "metadata": {
        "id": "69a09a53"
      },
      "source": [
        "We now know how to evaluate recommendations across our dataset quantitatively, but how can we actually print some recommendations for an example user based on the testset and compare this with their truly watched movies sorted by rating?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d32d8c87",
      "metadata": {
        "id": "d32d8c87"
      },
      "outputs": [],
      "source": [
        "def get_top_n(predictions, n=10):\n",
        "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
        "\n",
        "    Args:\n",
        "        predictions(list of Prediction objects): The list of predictions, as\n",
        "            returned by the test method of an algorithm.\n",
        "        n(int): The number of recommendation to output for each user. Default\n",
        "            is 10.\n",
        "\n",
        "    Returns:\n",
        "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
        "        [(raw item id, rating estimation), ...] of size n.\n",
        "    \"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b13b3171",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b13b3171",
        "outputId": "59441276-4924-44b5-ce00-047ddb5e581c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommendations for user with id 196:\n",
            "1 Lord of the Rings: The Two Towers, The (2002)\n",
            "2 Alien (1979)\n",
            "3 Mr. Holland's Opus (1995)\n",
            "4 Pianist, The (2002)\n",
            "5 Up (2009)\n"
          ]
        }
      ],
      "source": [
        "top_n = get_top_n(predictions, n=5)\n",
        "\n",
        "# Let's get recommendations for user 196\n",
        "uid = str(196)\n",
        "\n",
        "# collect all movies the user rated\n",
        "user_movies = ratings[ratings['userId'] == int(uid)]\n",
        "\n",
        "# get the recommendations for the specific user from the top_n dict\n",
        "recommendations = [movies[movies['movieId'] == int(iid)]['title'].iloc[0] for (iid, _) in top_n[uid]]\n",
        "\n",
        "print('Top 5 recommendations for user with id 196:')\n",
        "for i, rec in enumerate(recommendations):\n",
        "    print(f'{i+1} {rec}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54305f0",
      "metadata": {
        "id": "b54305f0"
      },
      "source": [
        "We can compare these recommendations with the actual movies found in the testset for that user ranked by rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5b414fad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b414fad",
        "outputId": "f463533b-90b8-4b8b-bc05-4be3eb58a27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. Holland's Opus (1995)\n",
            "Up (2009)\n",
            "Pianist, The (2002)\n",
            "Lord of the Rings: The Two Towers, The (2002)\n",
            "Alien (1979)\n",
            "The Martian (2015)\n"
          ]
        }
      ],
      "source": [
        "# First we collect all movies the user rated and filter them down to only those appearing in the test set\n",
        "user_movies = ratings[ratings['userId'] == uid].sort_values(by='rating', ascending=False)\n",
        "movies_relevant = []\n",
        "for user, movie, rating in testset:\n",
        "    if user == uid:\n",
        "        movies_relevant.append(movies[movies['movieId'] == int(movie)]['title'].iloc[0])\n",
        "\n",
        "for movie in movies_relevant:\n",
        "    print(movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47919dc0",
      "metadata": {
        "id": "47919dc0"
      },
      "source": [
        "# Hyperparameter estimation via GridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e48b8f5",
      "metadata": {
        "id": "8e48b8f5"
      },
      "source": [
        "As in data mining, the recommendation pipeline/model may have certain hyperparameters that need to be set by the user independently of the training process. As the evaluation setup is very similar to the data mining model learning phase, we can apply the same ideas here, for example, perform an automated hyperparameter search using Gridsearch with cross-validation on our training split.\n",
        "\n",
        "In the following cell, we perform a Gridsearch with cross-validation on the training set to optimise the SVD algorithm and then retrain the model on the full training set with the best found parameters and perform an unbiased evaluation on a withheld test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "22a79cf8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22a79cf8",
        "outputId": "813a19e6-5e88-4840-d61f-8cbb92535677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation score: 0.898470703958668\n",
            "Best found parameter setting: {'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
            "Biased accuracy on the full training set,   RMSE: 0.8574\n",
            "Unbiased accuracy on the test set, RMSE: 0.8897\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "raw_ratings = data.raw_ratings\n",
        "\n",
        "# shuffle ratings if you want\n",
        "random.shuffle(raw_ratings)\n",
        "\n",
        "# A = 80% of the data, B = 20% of the data\n",
        "threshold = int(0.8 * len(raw_ratings))\n",
        "A_raw_ratings = raw_ratings[:threshold]\n",
        "B_raw_ratings = raw_ratings[threshold:]\n",
        "\n",
        "data.raw_ratings = A_raw_ratings  # data is now the set A\n",
        "\n",
        "param_grid = {\"n_epochs\": [5, 10], \"lr_all\": [0.002, 0.005], \"reg_all\": [0.4, 0.6]}\n",
        "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\"], cv=3)\n",
        "\n",
        "gs.fit(data)\n",
        "\n",
        "# best RMSE score\n",
        "\n",
        "print(f'Best validation score: {gs.best_score[\"rmse\"]}')\n",
        "\n",
        "# combination of parameters that gave the best RMSE score\n",
        "print(f'Best found parameter setting: {gs.best_params[\"rmse\"]}')\n",
        "\n",
        "# Retrain on full train set with best settings\n",
        "algo = gs.best_estimator[\"rmse\"]\n",
        "# retrain on the whole set A\n",
        "trainset = data.build_full_trainset()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Compute biased accuracy on A\n",
        "predictions = algo.test(trainset.build_testset())\n",
        "print(\"Biased accuracy on the full training set,\", end=\"   \")\n",
        "rmse(predictions)\n",
        "\n",
        "# Compute unbiased accuracy on B\n",
        "testset = data.construct_testset(B_raw_ratings)  # testset is now the set B\n",
        "predictions = algo.test(testset)\n",
        "print(\"Unbiased accuracy on the test set,\", end=\" \")\n",
        "rmse(predictions);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}