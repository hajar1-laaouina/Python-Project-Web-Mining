{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-GjZHiHAJmq"
      },
      "source": [
        "We will use \"IMDB movie review sentiment classification dataset\"\n",
        "\n",
        "Dataset Description: https://keras.io/api/datasets/imdb/\n",
        "\n",
        "This is a dataset of 25,000 movie reviews from IMDB, tagged by sentiment (positive/negative). The reviews have been preprocessed and each review is coded as a list of (whole) word indexes. For convenience, words are indexed by their overall frequency in the dataset, so that, for example, the integer \"3\" encodes the 3rd most frequent word in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9OhvH-SAJm1",
        "outputId": "36f369a0-1313-4928-b7ea-ce4c00e5505c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "     ---------------------------------------- 1.7/1.7 MB 7.7 MB/s eta 0:00:00\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoWEej_jAJm_",
        "outputId": "68bcacf9-e3ac-472a-dee3-be776f81cf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
            "Collecting tensorflow-intel==2.11.0\n",
            "  Downloading tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
            "     -------------------------------------- 266.3/266.3 MB 3.1 MB/s eta 0:00:00\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
            "     --------------------------------------- 24.4/24.4 MB 10.7 MB/s eta 0:00:00\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
            "     ---------------------------------------- 1.5/1.5 MB 13.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.6)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
            "     ------------------------------------- 896.6/896.6 kB 14.1 MB/s eta 0:00:00\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "     -------------------------------------- 439.2/439.2 kB 9.1 MB/s eta 0:00:00\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "     -------------------------------------- 126.5/126.5 kB 7.3 MB/s eta 0:00:00\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "     ---------------------------------------- 6.0/6.0 MB 3.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "     ------------------------------------- 781.3/781.3 kB 12.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata in c:\\users\\pc\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.8.1)\n",
            "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, protobuf, opt-einsum, google-pasta, gast, astunparse, absl-py, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.1\n",
            "    Uninstalling protobuf-3.20.1:\n",
            "      Successfully uninstalled protobuf-3.20.1\n",
            "Successfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 libclang-16.0.0 opt-einsum-3.3.0 protobuf-3.20.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\pc\\appdata\\roaming\\python\\python37\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56nVYzL0AJnA",
        "outputId": "b1c497c1-1f9c-4e4f-d7ea-c0b044ec20a6"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'one_hot' from 'tensorflow.python.keras.utils' (C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18196\\348174041.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'one_hot' from 'tensorflow.python.keras.utils' (C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\__init__.py)"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM, Dropout\n",
        "from tensorflow.python.keras.layers.embeddings import Embedding\n",
        "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
        "from tensorflow.python.keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.utils import one_hot\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import Flatten\n",
        "# fix random seed for reproducibility;pl\n",
        "numpy.random.seed(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2DVsKDWAJnC"
      },
      "outputs": [],
      "source": [
        "db=imdb.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o986Mn6iAJnD"
      },
      "outputs": [],
      "source": [
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd87QTpAAJnE",
        "outputId": "66262b07-438e-43c7-fbb6-fa4ba7a3c9d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5sJmO6xAJnF",
        "outputId": "478b19a8-19ef-40d2-aad5-074cbf77eff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWXX7yJYAJnG"
      },
      "outputs": [],
      "source": [
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxQsQV5qAJnI",
        "outputId": "bb120067-84d5-497c-904c-c726f6e9fa43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 500)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkCJ8SA3AJnJ"
      },
      "source": [
        "we will use the embedding layer which defines the first hidden layer of the network. it must specify 3 arguments:\n",
        "\n",
        "input_dim: the size of the vocabulary in the text\n",
        "\n",
        "output_dim: this is the size of the vector space in which each word will be immersed\n",
        "\n",
        "input_legth: this is the size of the sequence, for example if your documents contain 100 words each then it is 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxn5A_biAJnK",
        "outputId": "46a62e69-034e-4905-b33c-361d6874979d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 216,405\n",
            "Trainable params: 216,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/6\n",
            "25000/25000 [==============================] - 185s 7ms/sample - loss: 0.4415 - acc: 0.7826 - val_loss: 0.3030 - val_acc: 0.8742\n",
            "Epoch 2/6\n",
            "25000/25000 [==============================] - 196s 8ms/sample - loss: 0.2538 - acc: 0.9012 - val_loss: 0.3165 - val_acc: 0.8664\n",
            "Epoch 3/6\n",
            "25000/25000 [==============================] - 197s 8ms/sample - loss: 0.2066 - acc: 0.9222 - val_loss: 0.2941 - val_acc: 0.8778\n",
            "Epoch 4/6\n",
            "25000/25000 [==============================] - 193s 8ms/sample - loss: 0.1739 - acc: 0.9346 - val_loss: 0.2926 - val_acc: 0.8828\n",
            "Epoch 5/6\n",
            "25000/25000 [==============================] - 196s 8ms/sample - loss: 0.1488 - acc: 0.9464 - val_loss: 0.3208 - val_acc: 0.8734\n",
            "Epoch 6/6\n",
            "25000/25000 [==============================] - 195s 8ms/sample - loss: 0.1225 - acc: 0.9575 - val_loss: 0.3376 - val_acc: 0.8810\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1ad71b91e80>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating tyhe model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=6, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F10QATrEAJnL",
        "outputId": "51a96671-5ecc-46c6-d989-a38054b93454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 88.10%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAUrfAJfAJnM"
      },
      "source": [
        "## a simple example of the embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWgJxe_kAJnN"
      },
      "outputs": [],
      "source": [
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzRQb4ACAJnO"
      },
      "outputs": [],
      "source": [
        "labels = [1,1,1,1,1,0,0,0,0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8E4xYscAJnO"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaIOREUBAJnP",
        "outputId": "cdb5917c-6cda-456c-ab7b-b63ce969dfaa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'one_hot' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18196\\3496244866.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoded_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18196\\3496244866.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoded_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'one_hot' is not defined"
          ]
        }
      ],
      "source": [
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_4qmGpgAJnQ",
        "outputId": "090267cd-1f18-4c61-ee4a-ecd5d65c43b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[12, 43], [7, 40], [28, 36], [1, 40], [5], [20], [3, 36], [16, 7], [3, 40], [37, 26, 43, 40]]\n"
          ]
        }
      ],
      "source": [
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV3wJc8pAJnR",
        "outputId": "9f008893-b2bc-4168-cf7b-c9cf866f3cac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[12 43  0  0]\n",
            " [ 7 40  0  0]\n",
            " [28 36  0  0]\n",
            " [ 1 40  0  0]\n",
            " [ 5  0  0  0]\n",
            " [20  0  0  0]\n",
            " [ 3 36  0  0]\n",
            " [16  7  0  0]\n",
            " [ 3 40  0  0]\n",
            " [37 26 43 40]]\n"
          ]
        }
      ],
      "source": [
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvPYvJ27AJnR"
      },
      "source": [
        "We are now ready to define our Embedding layer as part of our model.\n",
        "\n",
        "The embedding has a vocabulary of 50 and an entry length of 4. We will choose a small embedding space of 8 dimensions.\n",
        "\n",
        "The model is a simple binary classification model. It is important to note that the output of the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten it (the flatten layer) into a 32-element vector to pass it to the Dense output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg1A3Fz9AJnS",
        "outputId": "88c685a6-4abb-4584-9ef3-f50dd3dd183c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I54e6vgsAJnU",
        "outputId": "21262884-a85c-449c-ada8-114855c57591"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x19d3b571dd8>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(padded_docs, labels, epochs=50, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucEP4zdCAJnV",
        "outputId": "f9da63c6-a4af-40f8-c5ed-66cbee73dcd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 100.000000\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-UdWeE1AJnW"
      },
      "source": [
        "## To Do:\n",
        "\n",
        "1. Try the same thing on Google reviews dataset ( the file is given in the lab directory)\n",
        "2. try to change the embedding representation using Glove and Skipgram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Download GloVe word embeddings\n",
        "\n",
        "# Replace 'glove.6B.100d.txt' with the appropriate GloVe file name and path\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5HLxGj7dpDm",
        "outputId": "4026beb1-d85f-4c75-8300-f7b11bd4df53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-17 18:46:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-06-17 18:46:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-06-17 18:46:03--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2        8%[>                   ]  71.01M  5.10MB/s    eta 2m 2s  ^C\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FwTJeiOveFz5",
        "outputId": "15ec135e-7035-4e8d-cc68-bcdf32158f85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                userName                                          userImage  \\\n",
              "0          Andrew Thomas  https://lh3.googleusercontent.com/a-/AOh14GiHd...   \n",
              "1           Craig Haines  https://lh3.googleusercontent.com/-hoe0kwSJgPQ...   \n",
              "2          steven adkins  https://lh3.googleusercontent.com/a-/AOh14GiXw...   \n",
              "3       Lars Panzerbjørn  https://lh3.googleusercontent.com/a-/AOh14Gg-h...   \n",
              "4          Scott Prewitt  https://lh3.googleusercontent.com/-K-X1-YsVd6U...   \n",
              "...                  ...                                                ...   \n",
              "15741          Tammy Kay  https://lh3.googleusercontent.com/a-/AOh14GhYP...   \n",
              "15742          Ysm Johan  https://lh3.googleusercontent.com/a-/AOh14Ggmd...   \n",
              "15743      casey dearden  https://lh3.googleusercontent.com/a-/AOh14Gg2U...   \n",
              "15744     Jerry G Tamate  https://lh3.googleusercontent.com/a-/AOh14GiTP...   \n",
              "15745  Ahmed elsalamouni  https://lh3.googleusercontent.com/-9QSxVUhCoDI...   \n",
              "\n",
              "                                                 content  score  \\\n",
              "0      Update: After getting a response from the deve...      1   \n",
              "1      Used it for a fair amount of time without any ...      1   \n",
              "2      Your app sucks now!!!!! Used to be good but no...      1   \n",
              "3      It seems OK, but very basic. Recurring tasks n...      1   \n",
              "4      Absolutely worthless. This app runs a prohibit...      1   \n",
              "...                                                  ...    ...   \n",
              "15741  I believe that this is by far the best app wit...      5   \n",
              "15742                       It sometimes crashes a lot!!      5   \n",
              "15743                         Works well for what I need      5   \n",
              "15744                                           Love it.      5   \n",
              "15745  Really amazing and helped me sooo much just i ...      5   \n",
              "\n",
              "       thumbsUpCount reviewCreatedVersion                   at  \\\n",
              "0                 21             4.17.0.3  2020-04-05 22:25:57   \n",
              "1                 11             4.17.0.3  2020-04-04 13:40:01   \n",
              "2                 17             4.17.0.3  2020-04-01 16:18:13   \n",
              "3                192             4.17.0.2  2020-03-12 08:17:34   \n",
              "4                 42             4.17.0.2  2020-03-14 17:41:01   \n",
              "...              ...                  ...                  ...   \n",
              "15741              0                  NaN  2018-02-17 06:09:03   \n",
              "15742              0                4.3.7  2018-02-15 10:45:22   \n",
              "15743              0                4.3.7  2018-02-09 18:40:37   \n",
              "15744              0                  NaN  2018-02-06 12:36:17   \n",
              "15745              6                4.3.7  2018-02-04 22:57:09   \n",
              "\n",
              "                                            replyContent            repliedAt  \\\n",
              "0      According to our TOS, and the term you have ag...  2020-04-05 15:10:24   \n",
              "1      It sounds like you logged in with a different ...  2020-04-05 15:11:35   \n",
              "2      This sounds odd! We are not aware of any issue...  2020-04-02 16:05:56   \n",
              "3      We do offer this option as part of the Advance...  2020-03-15 06:20:13   \n",
              "4      We're sorry you feel this way! 90% of the app ...  2020-03-15 23:45:51   \n",
              "...                                                  ...                  ...   \n",
              "15741                                                NaN                  NaN   \n",
              "15742                                                NaN                  NaN   \n",
              "15743                                                NaN                  NaN   \n",
              "15744                                                NaN                  NaN   \n",
              "15745                                                NaN                  NaN   \n",
              "\n",
              "           sortOrder              appId  \n",
              "0      most_relevant          com.anydo  \n",
              "1      most_relevant          com.anydo  \n",
              "2      most_relevant          com.anydo  \n",
              "3      most_relevant          com.anydo  \n",
              "4      most_relevant          com.anydo  \n",
              "...              ...                ...  \n",
              "15741         newest  com.appxy.planner  \n",
              "15742         newest  com.appxy.planner  \n",
              "15743         newest  com.appxy.planner  \n",
              "15744         newest  com.appxy.planner  \n",
              "15745         newest  com.appxy.planner  \n",
              "\n",
              "[15746 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd055fe3-77c1-425e-a98f-23e792130626\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars Panzerbjørn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15741</th>\n",
              "      <td>Tammy Kay</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GhYP...</td>\n",
              "      <td>I believe that this is by far the best app wit...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2018-02-17 06:09:03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>newest</td>\n",
              "      <td>com.appxy.planner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15742</th>\n",
              "      <td>Ysm Johan</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Ggmd...</td>\n",
              "      <td>It sometimes crashes a lot!!</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4.3.7</td>\n",
              "      <td>2018-02-15 10:45:22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>newest</td>\n",
              "      <td>com.appxy.planner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15743</th>\n",
              "      <td>casey dearden</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg2U...</td>\n",
              "      <td>Works well for what I need</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4.3.7</td>\n",
              "      <td>2018-02-09 18:40:37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>newest</td>\n",
              "      <td>com.appxy.planner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15744</th>\n",
              "      <td>Jerry G Tamate</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiTP...</td>\n",
              "      <td>Love it.</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2018-02-06 12:36:17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>newest</td>\n",
              "      <td>com.appxy.planner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15745</th>\n",
              "      <td>Ahmed elsalamouni</td>\n",
              "      <td>https://lh3.googleusercontent.com/-9QSxVUhCoDI...</td>\n",
              "      <td>Really amazing and helped me sooo much just i ...</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>4.3.7</td>\n",
              "      <td>2018-02-04 22:57:09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>newest</td>\n",
              "      <td>com.appxy.planner</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15746 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd055fe3-77c1-425e-a98f-23e792130626')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd055fe3-77c1-425e-a98f-23e792130626 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd055fe3-77c1-425e-a98f-23e792130626');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTFjv8qIAJne",
        "outputId": "1099d773-7f94-46a1-ac16-a6614ea9fe85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "99/99 [==============================] - 1s 6ms/step - loss: -72.5791 - accuracy: 0.1612 - val_loss: -154.2550 - val_accuracy: 0.1632\n",
            "Epoch 2/10\n",
            "99/99 [==============================] - 1s 6ms/step - loss: -221.3691 - accuracy: 0.1624 - val_loss: -309.2115 - val_accuracy: 0.1632\n",
            "Epoch 3/10\n",
            "99/99 [==============================] - 1s 8ms/step - loss: -370.2920 - accuracy: 0.1624 - val_loss: -464.4019 - val_accuracy: 0.1632\n",
            "Epoch 4/10\n",
            "99/99 [==============================] - 1s 7ms/step - loss: -519.1991 - accuracy: 0.1624 - val_loss: -619.5007 - val_accuracy: 0.1632\n",
            "Epoch 5/10\n",
            "99/99 [==============================] - 1s 6ms/step - loss: -668.3745 - accuracy: 0.1624 - val_loss: -775.2603 - val_accuracy: 0.1632\n",
            "Epoch 6/10\n",
            "99/99 [==============================] - 1s 6ms/step - loss: -817.3159 - accuracy: 0.1624 - val_loss: -930.2280 - val_accuracy: 0.1632\n",
            "Epoch 7/10\n",
            "99/99 [==============================] - 0s 5ms/step - loss: -966.4356 - accuracy: 0.1624 - val_loss: -1085.8774 - val_accuracy: 0.1632\n",
            "Epoch 8/10\n",
            "99/99 [==============================] - 1s 6ms/step - loss: -1115.6232 - accuracy: 0.1624 - val_loss: -1240.5079 - val_accuracy: 0.1632\n",
            "Epoch 9/10\n",
            "99/99 [==============================] - 0s 5ms/step - loss: -1264.8484 - accuracy: 0.1624 - val_loss: -1396.4965 - val_accuracy: 0.1632\n",
            "Epoch 10/10\n",
            "99/99 [==============================] - 1s 6ms/step - loss: -1414.5249 - accuracy: 0.1624 - val_loss: -1552.0343 - val_accuracy: 0.1632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9082ad6f80>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "\n",
        "# Step 2: Load the Google reviews dataset\n",
        "\n",
        "# Replace 'path_to_csv' with the actual path to your Google reviews dataset\n",
        "df = pd.read_csv('reviews.csv')\n",
        "\n",
        "# Preprocess the data (tokenization, lowercase conversion, etc.)\n",
        "# Replace this with your own preprocessing steps\n",
        "\n",
        "# Split the data into training and test sets\n",
        "# Adjust the test_size and random_state as needed\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['score'], test_size=0.2, random_state=42)\n",
        "import numpy as np\n",
        "# Step 3: Tokenize the text data\n",
        "\n",
        "top_words = 5000\n",
        "tokenizer = Tokenizer(num_words=top_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_sequence_length = max(len(sequence) for sequence in X_train_sequences)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Step 4: Create an embedding matrix using GloVe word embeddings\n",
        "\n",
        "embedding_dim = 100  # Change this to the appropriate embedding dimension based on the GloVe file used\n",
        "\n",
        "# Create a dictionary mapping words to their corresponding GloVe vectors\n",
        "word_to_vec = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype='float32')\n",
        "        word_to_vec[word] = vector\n",
        "\n",
        "# Create an embedding matrix\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word_to_vec:\n",
        "        embedding_matrix[i] = word_to_vec[word]\n",
        "\n",
        "# Step 5: Build and train a model using Skipgram embedding representation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=10, batch_size=128)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}